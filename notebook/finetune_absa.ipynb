{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7Q3LGH_BlsP",
    "outputId": "3fc4cdd7-ce6a-48c6-d282-4b4c85c218d5"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install necessary packages\n",
    "!pip install transformers tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4sjZyGfBvqB"
   },
   "outputs": [],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    create_optimizer\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDqTPNelBx9a",
    "outputId": "71fc18ea-8599-4257-be83-b067e73e6051"
   },
   "outputs": [],
   "source": [
    "# Downloasd dataset\n",
    "\n",
    "!gdown https://drive.google.com/file/d/1DNgciEZTQWuo_wq4qMGIV8z3VC1Onjjv/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1ePfUhb2XeicnybAMu0y2w3BhiWm4XY8t/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1FRn6Gn3ijmCWfhUI6QkGm4g4LeQ7Veg0/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1Fw3yivW_PSH-C5T_4-9bWshxPHxvxtAi/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1F9r26d7emj43EG6kcMfvOsu1rI8yxR4d/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1I0vjwcnwfaMZdwhS-oJjqUoihl9NxZ1g/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1tX9mZBStZGCQmOhgaCu-tSdlXqQU77ou/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1-GNcBpiB3Y0RzbJLawxL31IjkTBtpUz2/view?usp=share_link --fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqm73fQbBx2x"
   },
   "outputs": [],
   "source": [
    "def load_mams_data(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df = df[['text', 'term', 'polarity']]\n",
    "  return df\n",
    "\n",
    "def load_laptop_data(path):\n",
    "  text_ds = []\n",
    "  term_ds = []\n",
    "  polarity_ds = []\n",
    "  with open(path, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "  terms = []\n",
    "  polarities = []\n",
    "  text = \"\"\n",
    "  term = \"\"\n",
    "  polarity = \"\"\n",
    "  for row in data[2:]:\n",
    "    # We are on new text column\n",
    "    if len(row) == 1:\n",
    "      if term != \"\":\n",
    "        terms.append(term)\n",
    "        polarities.append(polarity)\n",
    "      for i in range(len(terms)):\n",
    "        text_ds.append(text.strip())\n",
    "        term_ds.append(terms[i].strip())\n",
    "        polarity_ds.append(polarities[i])\n",
    "\n",
    "      text = \"\"\n",
    "      term = \"\"\n",
    "      polarity = \"\"\n",
    "      terms = []\n",
    "      polarities = []\n",
    "\n",
    "    else:\n",
    "      token, label = row.rsplit(',', 1)\n",
    "      text += \" \" + token\n",
    "\n",
    "      if label == 'T-POS':\n",
    "        label = 'positive'\n",
    "      elif label == 'T-NEG':\n",
    "        label = 'negative'\n",
    "      elif label == 'T-NEU':\n",
    "        label = 'neutral'\n",
    "      else:\n",
    "        if term != \"\":\n",
    "          terms.append(term)\n",
    "          polarities.append(polarity)\n",
    "        term = \"\"\n",
    "        polarity = \"\"\n",
    "        continue\n",
    "\n",
    "      if polarity == label:\n",
    "        term += \" \" + token\n",
    "\n",
    "      else:\n",
    "        if term != \"\":\n",
    "          terms.append(term)\n",
    "          polarities.append(polarity)\n",
    "\n",
    "        polarity = label\n",
    "        term = token\n",
    "\n",
    "  df = pd.DataFrame()\n",
    "  df['text'] = text_ds\n",
    "  df['term'] = term_ds\n",
    "  df['polarity'] = polarity_ds\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9ckxaXj_Bx1p",
    "outputId": "48713da0-5660-423f-e6f5-639e85816504"
   },
   "outputs": [],
   "source": [
    "mams_train = pd.read_csv(\"/content/mams_train.csv\")\n",
    "mams_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgYmKRskCRTe"
   },
   "outputs": [],
   "source": [
    "laptop_train, laptop_test, laptop_valid = list(map(load_laptop_data, ['laptop_train.csv', 'laptop_test.csv', 'laptop_dev.csv']))\n",
    "\n",
    "# restaurant dataset\n",
    "rest_train, rest_test, rest_valid = list(map(load_laptop_data, ['rest16_train.csv', 'rest16_test.csv', 'rest16_dev.csv']))\n",
    "\n",
    "# mams dataset\n",
    "mams_train, mams_test, mams_valid = list(map(load_mams_data, ['mams_train.csv', 'mams_test.csv', 'mams_val.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CYJChGEMCUKc",
    "outputId": "eb9ad826-e71c-47bd-91eb-e1ceffcd28ac"
   },
   "outputs": [],
   "source": [
    "mams_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZK1t_xnCUHs"
   },
   "outputs": [],
   "source": [
    "# dataset combined, shuffled\n",
    "train_data = pd.concat([laptop_train, rest_train, mams_train, laptop_test, rest_test, mams_test]).sample(frac = 1).reset_index(drop = True)\n",
    "valid_data = pd.concat([laptop_valid, rest_valid, mams_valid]).sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "CU3N0vPDCT_t",
    "outputId": "cfa0bee4-6a15-4ed5-a8cb-a74d49325fc4"
   },
   "outputs": [],
   "source": [
    "print('*'*30,'data ', '*'*30)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj-QuJ7xCdTu"
   },
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['polarity'] != 'neutral']\n",
    "\n",
    "valid_data = valid_data[valid_data['polarity'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "4595jD9nCfEG",
    "outputId": "9a15610d-ec9d-4015-f5ed-8aeeac9e39fe"
   },
   "outputs": [],
   "source": [
    "print('*'*30,'data ', '*'*30)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "7moq2xCPDZhV",
    "outputId": "665f02a8-8cb2-46b7-8d5c-e44a413fc9f2"
   },
   "outputs": [],
   "source": [
    "print('*'*30,'data ', '*'*30)\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "svYwH929FT4s",
    "outputId": "77678708-1cd6-408f-e1d4-0df62fbb3585"
   },
   "outputs": [],
   "source": [
    "# data processing, encoding labels, and lower casing the sentence text and aspect text\n",
    "\n",
    "pol2idx = { 'positive' : 1, 'negative' : 0}\n",
    "for data in [train_data, valid_data]:\n",
    "  data['polarity'] = data['polarity'].apply(lambda x:pol2idx[x])\n",
    "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "  data['term'] = data['term'].apply(lambda x: str(x).lower()).astype('str')\n",
    "\n",
    "  data = data.drop_duplicates([\"text\", \"term\", \"polarity\"])\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "ac55f10e7845437cb2e429d06c632bd7",
      "c4cf7c05954745468e978ecd740501e7",
      "117e95428b4e458bbce8e5452f2f0f30",
      "b8bbb07d9b70486ca474d107aeb1bf2e",
      "10521c7adc01494d98405a4ffbf03e76",
      "1f81c6acc2184900b4406d2f0fb84ef1",
      "4c2b709465fd400eb4eaaf975c01d14c",
      "de47925e995e42caae3b62099d162e93",
      "4956bb9071584d7fa2b9a7922ff8d752",
      "3deda6a9dae54a118da61dbebef2c2ae",
      "5e07469bac77460b821dc10a64f8af97",
      "ddd126f44b784b4c95b26286f2725555",
      "f7f08a8ded9e46afa2f08161a4422135",
      "9eaaf0735c69474aae72ff5056993642",
      "a58e0014023042c38baf171caf044f3b",
      "50f6cf3c0d594ddf975f83ec2b95a566",
      "da2acc28790140eaad4518fb00c3c34e",
      "9dc42bdb93c744f6b3036390c9a8d7ad",
      "92dfed1290ff4477b732f8b74536755c",
      "e078579f51534b9694846af1ee79a0ba",
      "92e8a8b3bb794805bdecf875ce83c1be",
      "b1bd099855ee405e8a1ae818b05cc35e",
      "bb6d4125c26f4301bd6d4417db3d1690",
      "cbfbd3bd389548fe8bc724d2f431f2b7",
      "0bccbc82ce194e318590cd7e854a186c",
      "89472e02bba04e84846f360d56eb58f9",
      "d3e057f9d74f403f8393b648e9a8bbd9",
      "a884198145864d3ab380bd2ef0881e48",
      "2a9584f5eee343cea9ce6c9c79d6f3d5",
      "9b698b5d957f451cb5736e802c9fb7a7",
      "bfa2844c0cf741268fc15dc1f8d42f60",
      "70aa6b9c0a8743508957f6a2c5bf2828",
      "b119fc3b4ff4424e8e0f6419437d0c78",
      "3b3549f07fc74253ab291a1aebf00029",
      "b64241cf738046b0994527efe23bfddc",
      "07d3bf80722b41269d90402a1cf95570",
      "f126a9a9b47a4f2c85b2f116b8b93418",
      "7864985a6d9643408d016121edb6d169",
      "290e85a87fcf4f1f827ad7d53a4951e3",
      "894d90ef932e4d68b68db0f708d5a9b9",
      "f44eb714d10e4fa98eccb3e88f4d1f4a",
      "f6585b2fba694ee3b8dd8df89a910150",
      "0ff45fd1594244fea08bae911f1d68bd",
      "770f1f2a791e4bb1be242434c3aacc00"
     ]
    },
    "id": "50AfY_8ECdP4",
    "outputId": "64cd8677-90d2-4b2f-95fe-764bd0818f11"
   },
   "outputs": [],
   "source": [
    "# Step 4: Initialize tokenizer and encode dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tC0Wn9SCdIX"
   },
   "outputs": [],
   "source": [
    "# Function to tokenize sentences\n",
    "def encode_sentence(sentence, aspect):\n",
    "    text = f\"[CLS] {aspect} [SEP] {sentence} [SEP]\"\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        max_length=100,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTF2OeTYDfiJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare TensorFlow dataset\n",
    "def encode_examples(df):\n",
    "    input_ids_list, attention_mask_list, label_list = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        encoded = encode_sentence(row['text'], row['term'])\n",
    "        input_ids_list.append(encoded['input_ids'])\n",
    "        attention_mask_list.append(encoded['attention_mask'])\n",
    "        label_list.append(row['polarity'])\n",
    "\n",
    "    return (\n",
    "        tf.concat(input_ids_list, axis=0),\n",
    "        tf.concat(attention_mask_list, axis=0),\n",
    "        tf.convert_to_tensor(label_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHVeR8v_D4r9"
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_labels = encode_examples(train_data)\n",
    "val_input_ids, val_attention_masks, val_labels = encode_examples(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r20hot3ID4ip",
    "outputId": "98c1fe36-840f-4ed4-dfba-01ef9e98e259"
   },
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7qHpJk7DfW2"
   },
   "outputs": [],
   "source": [
    "# Step 5: Create TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_input_ids, 'attention_mask': train_attention_masks},\n",
    "    train_labels\n",
    ")).shuffle(1000).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaIviovmGJ8p"
   },
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': val_input_ids, 'attention_mask': val_attention_masks},\n",
    "    val_labels\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "5ca644582c764d28a875a140cbf5fa16",
      "84e5f562685d435aac55e8de8bf837e4",
      "f7da7e0bccc84f7b8197bb05f5935436",
      "46022e78c107475ba9d67b4ea606bcd9",
      "825b850468ea4c26bbca5aab1ad0c04b",
      "2cce574f54884832a7c2b7337e80b86c",
      "1926cad23a334ab9aede9eb4446d196d",
      "be9e8cb8d2f14353b57c07dbc08675fe",
      "af1f99269e5d424496e2e61fbf8757ed",
      "8f29113331c5482ba4f93087f2cbdb60",
      "96b22a925f8f4b3e852ab0c7f01b8a2d"
     ]
    },
    "id": "d19XK3fOGJ2T",
    "outputId": "2f655bad-a207-4380-be0f-957989bf8f1c"
   },
   "outputs": [],
   "source": [
    "# Step 6: Load BERT model with classification head\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inSpxYv9FomH"
   },
   "outputs": [],
   "source": [
    "# Step 7: Compile the model\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_train_steps=5000, num_warmup_steps=500)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxLTvOTiGPqk",
    "outputId": "75aad77c-3754-48ac-f62d-66c833d3d4a6"
   },
   "outputs": [],
   "source": [
    "# Step 8: Train the model\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlGys63dGPjU",
    "outputId": "2c1e1f92-5e6b-4092-9181-be87f191875f"
   },
   "outputs": [],
   "source": [
    "# Step 9: Save fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_absa')\n",
    "tokenizer.save_pretrained('fine_tuned_bert_absa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5td_ojOQA8j",
    "outputId": "e7ba1d95-e721-4247-99d3-bb90864ce854"
   },
   "outputs": [],
   "source": [
    "# Step 10: Load the fine-tuned model and tokenizer\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained('fine_tuned_bert_absa')\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained('fine_tuned_bert_absa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO-nTg-LQHy0"
   },
   "outputs": [],
   "source": [
    "# Step 13: Make predictions on sample data\n",
    "def predict_sentiment(sentence, aspect):\n",
    "    encoded = loaded_tokenizer.encode_plus(\n",
    "        f\"[CLS] {aspect.lower()} [SEP] {sentence.lower()} [SEP]\",\n",
    "        add_special_tokens=False,\n",
    "        max_length=100,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "\n",
    "    predictions = loaded_model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
    "    predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "    idx2pol = {0: 'negative', 1: 'positive'}\n",
    "    return idx2pol[predicted_class]\n",
    "\n",
    "# Example predictions\n",
    "sample_sentence1 = \"The food was amazing, but the service was slow.\"\n",
    "sample_aspect1 = \"food\"\n",
    "sample_aspect2 = \"service\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18WETCxBQkH_",
    "outputId": "5ce1c99e-d842-4729-c3af-ac5b301aed90"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSentence: '{sample_sentence1}'\")\n",
    "print(f\"Aspect: '{sample_aspect1}' -> Predicted Sentiment: {predict_sentiment(sample_sentence1, sample_aspect1)}\")\n",
    "print(f\"Aspect: '{sample_aspect2}' -> Predicted Sentiment: {predict_sentiment(sample_sentence1, sample_aspect2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de2bm5khQkCD",
    "outputId": "b1982ffc-5b76-4b4c-890f-41a7434dbf54"
   },
   "outputs": [],
   "source": [
    "sample_sentence2 = \"The battery life is terrible on this laptop.\"\n",
    "sample_aspect3 = \"battery life\"\n",
    "print(f\"\\nSentence: '{sample_sentence2}'\")\n",
    "print(f\"Aspect: '{sample_aspect3}' -> Predicted Sentiment: {predict_sentiment(sample_sentence2, sample_aspect3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIFIk6--QkA4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1S0Ll8GRQj7I",
    "outputId": "94683bc5-a3be-4a21-c36b-929340722702"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive('my_folder', 'zip', '/content/fine_tuned_bert_absa')  # (output_name, format, folder_path)\n",
    "\n",
    "# Download the zipped folder\n",
    "files.download('my_folder.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcrxzubkQ4bi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhCn-VQtQ4Vq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "HUPdSG8ZMil7",
    "outputId": "85c354f6-2362-4bc4-f42f-5c5e3672674b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assume y_true are the actual labels, y_pred are the predicted labels,\n",
    "# and y_logits are the raw output logits from BERT.\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# For ROC-AUC, use probabilities or logits converted to probabilities\n",
    "probabilities = tf.nn.softmax(y_logits, axis=-1)[:, 1]  # Probabilities for positive class\n",
    "roc_auc = roc_auc_score(y_true, probabilities)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
