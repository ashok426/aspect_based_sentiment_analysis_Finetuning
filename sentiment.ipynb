{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f75c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from tensorflow) (4.14.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.72.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ashok/miniconda3/envs/nlp_cpu_env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "Using cached grpcio-1.72.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, astunparse, absl-py, werkzeug, smart-open, scipy, requests, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, gensim, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/35\u001b[0m [tensorflow]5\u001b[0m [tensorflow]]data-server]stem]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.0 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 gensim-4.3.3 google-pasta-0.2.0 grpcio-1.72.1 h5py-3.14.0 idna-3.10 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-1.26.4 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 requests-2.32.3 rich-14.0.0 scipy-1.13.1 smart-open-7.1.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 urllib3-2.4.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d27db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce89348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca706fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369f89dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1DNgciEZTQWuo_wq4qMGIV8z3VC1Onjjv\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/laptop_dev.csv\n",
      "100%|███████████████████████████████████████| 46.5k/46.5k [00:00<00:00, 812kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ePfUhb2XeicnybAMu0y2w3BhiWm4XY8t\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/laptop_test.csv\n",
      "100%|██████████████████████████████████████| 98.6k/98.6k [00:00<00:00, 1.08MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FRn6Gn3ijmCWfhUI6QkGm4g4LeQ7Veg0\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/laptop_train.csv\n",
      "100%|████████████████████████████████████████| 378k/378k [00:00<00:00, 2.45MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Fw3yivW_PSH-C5T_4-9bWshxPHxvxtAi\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/mams_test.csv\n",
      "100%|████████████████████████████████████████| 222k/222k [00:00<00:00, 1.73MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1F9r26d7emj43EG6kcMfvOsu1rI8yxR4d\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/mams_train.csv\n",
      "100%|██████████████████████████████████████| 1.78M/1.78M [00:00<00:00, 7.55MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zSYr8akNxtCCR_TxVIFrel1k254k-vin\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/mams_val.csv\n",
      "100%|████████████████████████████████████████| 219k/219k [00:00<00:00, 1.70MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zSYr8akNxtCCR_TxVIFrel1k254k-vin\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/mams_val.csv\n",
      "100%|████████████████████████████████████████| 219k/219k [00:00<00:00, 1.68MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zSYr8akNxtCCR_TxVIFrel1k254k-vin\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/mams_val.csv\n",
      "100%|████████████████████████████████████████| 219k/219k [00:00<00:00, 1.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1I0vjwcnwfaMZdwhS-oJjqUoihl9NxZ1g\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/rest16_train.csv\n",
      "100%|████████████████████████████████████████| 221k/221k [00:00<00:00, 1.80MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tX9mZBStZGCQmOhgaCu-tSdlXqQU77ou\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/rest16_dev.csv\n",
      "100%|██████████████████████████████████████| 24.3k/24.3k [00:00<00:00, 1.29MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-GNcBpiB3Y0RzbJLawxL31IjkTBtpUz2\n",
      "To: /home/ashok/aspect_based_sentiment_analysis/rest16_test.csv\n",
      "100%|██████████████████████████████████████| 84.1k/84.1k [00:00<00:00, 1.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Downloasd dataset\n",
    "\n",
    "!gdown https://drive.google.com/file/d/1DNgciEZTQWuo_wq4qMGIV8z3VC1Onjjv/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1ePfUhb2XeicnybAMu0y2w3BhiWm4XY8t/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1FRn6Gn3ijmCWfhUI6QkGm4g4LeQ7Veg0/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1Fw3yivW_PSH-C5T_4-9bWshxPHxvxtAi/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1F9r26d7emj43EG6kcMfvOsu1rI8yxR4d/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1zSYr8akNxtCCR_TxVIFrel1k254k-vin/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1I0vjwcnwfaMZdwhS-oJjqUoihl9NxZ1g/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1tX9mZBStZGCQmOhgaCu-tSdlXqQU77ou/view?usp=share_link --fuzzy\n",
    "!gdown https://drive.google.com/file/d/1-GNcBpiB3Y0RzbJLawxL31IjkTBtpUz2/view?usp=share_link --fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b7d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mams_data(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df = df[['text', 'term', 'polarity']]\n",
    "  return df\n",
    "\n",
    "def load_laptop_data(path):\n",
    "  text_ds = []\n",
    "  term_ds = []\n",
    "  polarity_ds = []\n",
    "  with open(path, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "  terms = []\n",
    "  polarities = []\n",
    "  text = \"\"\n",
    "  term = \"\"\n",
    "  polarity = \"\"\n",
    "  for row in data[2:]:\n",
    "    # We are on new text column\n",
    "    if len(row) == 1:\n",
    "      if term != \"\":\n",
    "        terms.append(term)\n",
    "        polarities.append(polarity)\n",
    "      for i in range(len(terms)):\n",
    "        text_ds.append(text.strip())\n",
    "        term_ds.append(terms[i].strip())\n",
    "        polarity_ds.append(polarities[i])\n",
    "\n",
    "      text = \"\"\n",
    "      term = \"\"\n",
    "      polarity = \"\"\n",
    "      terms = []\n",
    "      polarities = []\n",
    "\n",
    "    else:\n",
    "      token, label = row.rsplit(',', 1)\n",
    "      text += \" \" + token\n",
    "\n",
    "      if label == 'T-POS':\n",
    "        label = 'positive'\n",
    "      elif label == 'T-NEG':\n",
    "        label = 'negative'\n",
    "      elif label == 'T-NEU':\n",
    "        label = 'neutral'\n",
    "      else:\n",
    "        if term != \"\":\n",
    "          terms.append(term)\n",
    "          polarities.append(polarity)\n",
    "        term = \"\"\n",
    "        polarity = \"\"\n",
    "        continue\n",
    "\n",
    "      if polarity == label:\n",
    "        term += \" \" + token\n",
    "\n",
    "      else:\n",
    "        if term != \"\":\n",
    "          terms.append(term)\n",
    "          polarities.append(polarity)\n",
    "\n",
    "        polarity = label\n",
    "        term = token\n",
    "\n",
    "  df = pd.DataFrame()\n",
    "  df['text'] = text_ds\n",
    "  df['term'] = term_ds\n",
    "  df['polarity'] = polarity_ds\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3c933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_train, laptop_test, laptop_valid = list(map(load_laptop_data, ['laptop_train.csv', 'laptop_test.csv', 'laptop_dev.csv']))\n",
    "\n",
    "# restaurant dataset\n",
    "rest_train, rest_test, rest_valid = list(map(load_laptop_data, ['rest16_train.csv', 'rest16_test.csv', 'rest16_dev.csv']))\n",
    "\n",
    "# mams dataset\n",
    "mams_train, mams_test, mams_valid = list(map(load_mams_data, ['mams_train.csv', 'mams_test.csv', 'mams_val.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e302569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset combined, shuffled\n",
    "train_data = pd.concat([laptop_train, rest_train, mams_train, laptop_test, rest_test, mams_test]).sample(frac = 1).reset_index(drop = True)\n",
    "valid_data = pd.concat([laptop_valid, rest_valid, mams_valid]).sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc3183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** data  ******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last time, the waiter told my roommate he'd ha...</td>\n",
       "      <td>waiter</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After ordering my dinner and having it come to...</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sort of crouched in a chair, leaning over a ta...</td>\n",
       "      <td>table</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It gets stuck all of the time you use it \",\" a...</td>\n",
       "      <td>work</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The kitchen would do well to offer more desser...</td>\n",
       "      <td>Rice connotes</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           term  polarity\n",
       "0  last time, the waiter told my roommate he'd ha...         waiter  negative\n",
       "1  After ordering my dinner and having it come to...      mushrooms  negative\n",
       "2  sort of crouched in a chair, leaning over a ta...          table   neutral\n",
       "3  It gets stuck all of the time you use it \",\" a...           work  negative\n",
       "4  The kitchen would do well to offer more desser...  Rice connotes  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*'*30,'data ', '*'*30)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a0cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['polarity'] != 'neutral']\n",
    "\n",
    "valid_data = valid_data[valid_data['polarity'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676311d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last time, the waiter told my roommate he'd ha...</td>\n",
       "      <td>waiter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after ordering my dinner and having it come to...</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it gets stuck all of the time you use it \",\" a...</td>\n",
       "      <td>work</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the kitchen would do well to offer more desser...</td>\n",
       "      <td>rice connotes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>park your trench coat at the coat check and be...</td>\n",
       "      <td>cuisine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           term  polarity\n",
       "0  last time, the waiter told my roommate he'd ha...         waiter         0\n",
       "1  after ordering my dinner and having it come to...      mushrooms         0\n",
       "3  it gets stuck all of the time you use it \",\" a...           work         0\n",
       "4  the kitchen would do well to offer more desser...  rice connotes         1\n",
       "6  park your trench coat at the coat check and be...        cuisine         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data processing, encoding labels, and lower casing the sentence text and aspect text\n",
    "\n",
    "pol2idx = { 'positive' : 1, 'negative' : 0}\n",
    "for data in [train_data, valid_data]:\n",
    "  data['polarity'] = data['polarity'].apply(lambda x:pol2idx[x])\n",
    "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "  data['term'] = data['term'].apply(lambda x: str(x).lower()).astype('str')\n",
    "\n",
    "  data = data.drop_duplicates([\"text\", \"term\", \"polarity\"])\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b070306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "# load word embeddings from gensim api\n",
    "\n",
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397067d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ashok/gensim-data\n"
     ]
    }
   ],
   "source": [
    "print(api.BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62bf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
